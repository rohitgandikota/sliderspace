{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e315634-43fe-4d55-8dbe-ab09125ab01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from typing import List, Optional\n",
    "import argparse\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from diffusers import DiffusionPipeline, UNet2DConditionModel, LCMScheduler, AutoencoderTiny\n",
    "from huggingface_hub import hf_hub_download\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "import os, glob\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import time, datetime\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from contextlib import ExitStack\n",
    "from safetensors.torch import load_file\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from transformers import CLIPModel\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.lora import LoRANetwork, DEFAULT_TARGET_REPLACE, UNET_TARGET_REPLACE_MODULE_CONV\n",
    "from utils.inference_util import StableDiffusionXLPipelineSliders\n",
    "from transformers import logging\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "from diffusers import logging\n",
    "logging.set_verbosity_error()\n",
    "modules = DEFAULT_TARGET_REPLACE\n",
    "modules += UNET_TARGET_REPLACE_MODULE_CONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621908bd-2f8d-499b-af5d-dd7b67951929",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "repo_name = \"tianweiy/DMD2\"\n",
    "ckpt_name = \"dmd2_sdxl_4step_unet_fp16.bin\"\n",
    "\n",
    "\n",
    "device = 'cuda:0'\n",
    "weight_dtype = torch.bfloat16\n",
    "\n",
    "# If you want to compose multiple sliders - please use numsliders_to_sample > 1\n",
    "numsliders_to_sample = 1\n",
    "\n",
    "\n",
    "unet = UNet2DConditionModel.from_config(base_model_id, subfolder=\"unet\").to(device, weight_dtype)\n",
    "\n",
    "unet.load_state_dict(torch.load(hf_hub_download(repo_name, ckpt_name)))\n",
    "\n",
    "pipe = StableDiffusionXLPipelineSliders.from_pretrained(base_model_id, unet=unet, torch_dtype=weight_dtype)\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe = pipe.to(device).to(weight_dtype)\n",
    "unet = pipe.unet\n",
    "\n",
    "## Change these parameters based on how you trained your sliderspace sliders\n",
    "train_method = 'xattn-strict'\n",
    "rank = 1 \n",
    "alpha =1 \n",
    "print(rank, alpha, train_method)\n",
    "networks = {}\n",
    "modules = DEFAULT_TARGET_REPLACE\n",
    "modules += UNET_TARGET_REPLACE_MODULE_CONV\n",
    "for i in range(numsliders_to_sample):\n",
    "    networks[i] = LoRANetwork(\n",
    "        unet,\n",
    "        rank=int(rank),\n",
    "        multiplier=1.0,\n",
    "        alpha=int(alpha),\n",
    "        train_method=train_method,\n",
    "        fast_init=True,\n",
    "    ).to(device, dtype=weight_dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95072d2d-f638-4f29-8c33-151e278ecc33",
   "metadata": {},
   "source": [
    "# Iterate through every slider you discovered and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512e1fc-76a6-46e1-9a0c-7309f743a951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = 'image of a robot'\n",
    "sliderspace_path = '../trained_sliders/sdxl/robot/'\n",
    "slider_scales = [2]\n",
    "num_images = 6\n",
    "sliderspace = glob.glob(f'{sliderspace_path}/*.pt')\n",
    "\n",
    "# seeds = [random.randint(0,2**15) for _ in range(num_images)]\n",
    "for slider_idx, slider in enumerate(sliderspace):\n",
    "    image_list = []\n",
    "    for net in networks:\n",
    "        networks[net].load_state_dict(torch.load(slider))\n",
    "\n",
    "    for im in range(num_images):\n",
    "        seed = seeds[im]\n",
    "        for scale in slider_scales:\n",
    "            for net in networks:\n",
    "                networks[net].set_lora_slider(scale)\n",
    "            \n",
    "            generator = torch.manual_seed(seed)\n",
    "\n",
    "            images = pipe(prompt, num_images_per_prompt=1, num_inference_steps=4, guidance_scale=0, generator=generator,\n",
    "                         networks=networks,\n",
    "                         # FROM WHAT TIMESTEP OF TOTAL INFERENE STEPS DO YOU wANT TO APPLY SLIDER? \n",
    "                         # INCREASE TO HAVE MORE PRECISE EDITS \n",
    "                         # (NOTE: FOR DISTILLED MODELS IT MIGHT NOT SHOW FULL SLIDER CAPACITY DUE TO THE FIRST TIMESTEP BEING SO POWERFUL)\n",
    "                         apply_sliders_from=0, \n",
    "                         apply_sliders_till=None, # LEAVE THIS NONE. UNLESS YOU WANT TO SKIP APPLYING SLIDER AT FINAL TIMESTEPS?\n",
    "                         ).images[0]\n",
    "            image_list.append(images)\n",
    "\n",
    "    print(f\"Slider {os.path.basename(slider).replace('.pt','').split('_')[-1]}\")\n",
    "    fig, ax = plt.subplots(1, len(image_list),frameon=False, dpi=300)\n",
    "    fig.set_size_inches(num_images * 3, 3)  # Adjust multiplier as needed\n",
    "\n",
    "    for i, a in enumerate(ax):\n",
    "        a.imshow(image_list[i])\n",
    "        a.axis('off')\n",
    "        a.set_position([0, 0, 1, 1])\n",
    "    \n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "    # Ensure the figure has no padding\n",
    "    fig.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9497fc-4f3b-46fc-b7f9-bed954b45993",
   "metadata": {},
   "source": [
    "# Make Gifs by sliding the slider scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db83780-c9a3-465a-a746-65cee59fa16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_smooth_gif(\n",
    "    pipe,\n",
    "    networks,\n",
    "    prompt,\n",
    "    slider_path,\n",
    "    num_frames=30,\n",
    "    scale_start=0,\n",
    "    scale_end=2,\n",
    "    seed=None,\n",
    "    output_path=\"interpolation.gif\",\n",
    "    fps=10,\n",
    "    dpi=300,\n",
    "):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    for net in networks:\n",
    "        networks[net].load_state_dict(torch.load(slider_path))\n",
    "    \n",
    "    # Simple linear interpolation for forward frames\n",
    "    scales = np.linspace(scale_start, scale_end, num_frames)\n",
    "    \n",
    "    images = []\n",
    "    seed = random.randint(0, 2**15)\n",
    "    pipe.set_progress_bar_config(disable=True)\n",
    "    \n",
    "    for scale in tqdm(scales):\n",
    "        for net in networks:\n",
    "            networks[net].set_lora_slider(float(scale))\n",
    "        \n",
    "        generator = torch.manual_seed(seed)\n",
    "\n",
    "        image = pipe(\n",
    "            prompt,\n",
    "            num_images_per_prompt=1,\n",
    "            num_inference_steps=4,\n",
    "            guidance_scale=0,\n",
    "            generator=generator,\n",
    "            networks=networks,\n",
    "             # FROM WHAT TIMESTEP OF TOTAL INFERENE STEPS DO YOU wANT TO APPLY SLIDER? \n",
    "             # INCREASE TO HAVE MORE PRECISE EDITS \n",
    "             # (NOTE: FOR DISTILLED MODELS IT MIGHT NOT SHOW FULL SLIDER CAPACITY DUE TO THE FIRST TIMESTEP BEING SO POWERFUL)\n",
    "             apply_sliders_from=1, \n",
    "             apply_sliders_till=None, # LEAVE THIS NONE. UNLESS YOU WANT TO SKIP APPLYING SLIDER AT FINAL TIMESTEPS?\n",
    "        ).images[0]\n",
    "            \n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(np.uint8(image))\n",
    "        \n",
    "        # Resize image while maintaining aspect ratio\n",
    "        image.thumbnail((256, 256), Image.Resampling.LANCZOS)\n",
    "        images.append(image)\n",
    "    \n",
    "    duration = 1000 / fps  # Convert fps to milliseconds\n",
    "    images[0].save(\n",
    "        output_path,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=duration,\n",
    "        loop=0,\n",
    "        optimize=True,\n",
    "        quality=70,\n",
    "        dpi=dpi\n",
    "    )\n",
    "    pipe.set_progress_bar_config(disable=False)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fb780-5c73-4064-9090-7c774b9236c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sliderspace_path = '../trained_sliders/sdxl/ancient ruins/'\n",
    "# prompt = 'picture of a ancient ruin'\n",
    "\n",
    "slider_scale_start = 0 \n",
    "slider_scale_end = 2\n",
    "\n",
    "num_frames = 50 # how many scales you want to interpolate between the start and end scale\n",
    "num_images = 2\n",
    "    \n",
    "sliderspace = glob.glob(f'{sliderspace_path}/*.pt')\n",
    "gifs_path = sliderspace_path.replace('trained_sliders','gifs')\n",
    "\n",
    "os.makedirs(gifs_path, exist_ok=True)\n",
    "\n",
    "seeds = [random.randint(0, 2**15) for _ in range(num_images)]\n",
    "print(seeds)\n",
    "\n",
    "for slider_idx, slider in enumerate(sliderspace):\n",
    "    slider_number = int(os.path.basename(slider).split('.')[0].split('_')[-1])\n",
    "\n",
    "    print(os.path.basename(slider))\n",
    "    for seed in seeds:\n",
    "        \n",
    "        output_filename = f\"{gifs_path}/{os.path.basename(slider).replace('.pt','_'+str(seed)+'.gif')}\"\n",
    "        \n",
    "        images = generate_smooth_gif(\n",
    "            pipe=pipe,\n",
    "            networks=networks,\n",
    "            prompt=prompt,\n",
    "            slider_path=slider,\n",
    "            num_frames=num_frames,  # Adjust for smoother/faster interpolation\n",
    "            scale_start=slider_scale_start,\n",
    "            scale_end=slider_scale_end,\n",
    "            output_path=output_filename,\n",
    "            seed = seed,\n",
    "            fps=20  # Adjust for slower/faster playback\n",
    "        )\n",
    "        \n",
    "        # Optional: Display first and last frame for verification\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax1.imshow(images[0])\n",
    "        ax1.set_title(\"Start (Scale = 0)\")\n",
    "        ax1.axis('off')\n",
    "        ax2.imshow(images[-1])\n",
    "        ax2.set_title(\"End (Scale = 1)\")\n",
    "        ax2.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de1696-c6a6-425e-a790-b2f96bfa56ef",
   "metadata": {},
   "source": [
    "# Generate Images with Multiple Sliders Composed\n",
    "\n",
    "## Make sure that you initialized more than 1 slider \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327c6f4-7315-4dfd-9d6d-7028b78200e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'picture of a spaceship'\n",
    "sliderspace_path = '../trained_sliders/spaceship/'\n",
    "\n",
    "slider_scale = 1\n",
    "num_images = 10\n",
    "sliderspace = glob.glob(f'{sliderspace_path}/*.pt')\n",
    "image_list = []\n",
    "for idx in range(num_images):\n",
    "    seed = random.randint(0,2**15)\n",
    "    generator = torch.manual_seed(seed)\n",
    "    sliderspace_samples = random.sample(sliderspace, numsliders_to_sample)\n",
    "    for i, net in enumerate(networks):\n",
    "        networks[net].load_state_dict(torch.load(sliderspace_samples[i]))\n",
    "        networks[net].set_lora_slider(slider_scale)\n",
    "        with networks[net]:\n",
    "            pass\n",
    "        \n",
    "    with ExitStack() as es:\n",
    "        for net in networks:\n",
    "            es.enter_context(networks[net])\n",
    "        images = pipe(prompt, num_images_per_prompt=1, num_inference_steps=4, guidance_scale=0, generator=generator,\n",
    "                     networks=networks,\n",
    "                     # FROM WHAT TIMESTEP OF TOTAL INFERENE STEPS DO YOU wANT TO APPLY SLIDER? \n",
    "                     # INCREASE TO HAVE MORE PRECISE EDITS \n",
    "                     # (NOTE: FOR DISTILLED MODELS IT MIGHT NOT SHOW FULL SLIDER CAPACITY DUE TO THE FIRST TIMESTEP BEING SO POWERFUL)\n",
    "                     apply_sliders_from=1, \n",
    "                     apply_sliders_till=None, # LEAVE THIS NONE. UNLESS YOU WANT TO SKIP APPLYING SLIDER AT FINAL TIMESTEPS?).images[0]\n",
    "                     ).images[0]\n",
    "    image_list.append(images)\n",
    "\n",
    "print('Randomly Sampled and Composed Sliders')\n",
    "fig, ax = plt.subplots(1, len(image_list),frameon=False, dpi=600)\n",
    "for i, a in enumerate(ax):\n",
    "    a.imshow(image_list[i])\n",
    "    a.axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
